{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DM8kLxUEVc3Z"
   },
   "source": [
    "# Natural Language Processing Demystified | Preprocessing\n",
    "## Sources\n",
    "Largely based on\n",
    "https://nlpdemystified.org<br>\n",
    "https://github.com/nitinpunjabi/nlp-demystified\n",
    "\n",
    "See also their great NLP course for more detail and explanations\n",
    "https://www.nlpdemystified.org/course/tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Installation and dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7Ll-fUK2VZs"
   },
   "source": [
    "At the time this notebook was created, spaCy had newer releases but Colab was still using version 2.x by default. So the first step is to upgrade spaCy.\n",
    "<br><br>\n",
    "**IMPORTANT**<br>\n",
    "If you're running this in the cloud rather than using a local Jupyter server on your machine, then the notebook will **timeout** after a period of inactivity. If that happens and you don't reconnect in time, you will need to upgrade spaCy again and reinstall the requisite statistical packages.\n",
    "<br><br>\n",
    "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
    "https://research.google.com/colaboratory/local-runtimes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vW9svTE289D"
   },
   "outputs": [],
   "source": [
    " import spacy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfJKSJEU2U_s"
   },
   "source": [
    "After importing spaCy, the next thing we need to do is load a suitable statistical model for our project. spaCy offers a variety of models for different languages. These models help with tokenization, part-of-speech tagging, named entity recognition, and more.\n",
    "\n",
    "Here, we're loading the **en_core_web_sm** model which is the smallest English model spaCy offers and is a good starting point for NLP tasks.<br>\n",
    "This model should've already been downloaded in by running the Ch0 notebook. If not, download it now by running\n",
    "```\n",
    "!python -m spacy download en_core_web_sm\n",
    "```\n",
    "https://spacy.io/models/en#en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWDrpxDk2_r2"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K7YCbWtG3LJO"
   },
   "source": [
    "**en_core_web_sm** is trained on OntoNotes 5 which is an annotated corpus comprising news, blogs, transcripts, etc. Put simply, this means a bunch of documents were labelled with information such as how each sentence should be parsed, whether a particular word is a noun or adjective or other part-of-speech, whether a word is a special entity like a person or a real-world organization, and other language-related labels. A statistical model was then generated from these labelled documents.<br>\n",
    "https://catalog.ldc.upenn.edu/LDC2013T19\n",
    "<br><br>\n",
    "You can learn more about the available spaCy models at these links:<br>\n",
    "https://spacy.io/models<br>\n",
    "https://spacy.io/usage/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvF_udvi3OTO"
   },
   "source": [
    "After loading the model, the _nlp_ variable now references a **Language** class instance which contains language-specific rules for various tasks (e.g. tokenization) and a processing pipeline.<br>\n",
    "https://spacy.io/api/language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAYGtQpT3UNN"
   },
   "outputs": [],
   "source": [
    "type(nlp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unmnGRu8D-wa"
   },
   "source": [
    "# Tokenization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just so you can start here and skip the installation part when you did that before.\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLUcGm3IbQki"
   },
   "source": [
    "### Tokenization with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13twUCp2i_p8"
   },
   "source": [
    "We pass whatever text we want to process to _nlp_, which returns a **Doc** container object containing the tokenized text and a number of annotations for each token. You can learn more about the **Doc** object here:<br>\n",
    "https://spacy.io/api/doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BIoEJZ-IkHQ4"
   },
   "outputs": [],
   "source": [
    "# Sample sentence.\n",
    "s = \"It's not about the money (only $20.15), it's about sending a message :). üöÄüíéüôå\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMWZK3ZSk9-f"
   },
   "source": [
    "We can iterate over this **Doc** object and view the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8SzqhZuulAe1"
   },
   "outputs": [],
   "source": [
    "print([t.text for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ai1obkB93GdD"
   },
   "source": [
    "Note how\n",
    "- `it's` is separated in `it` and `'s`\n",
    "- the currency symbol and amount are separated.\n",
    "- punctuation like `.` is separated when it has a function at the end of the sentence\n",
    "- punctuation like `.` is not separated when it is an indivisible part of a token\n",
    "- the period at the end of the sentence is its own token\n",
    "- emoji's are one token each"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWH49gIh3hqN"
   },
   "source": [
    "The **Doc** object can be indexed and sliced like a regular list. The **Doc** object contains **Token** and **Span** objects, which offer different views into the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MwLrxRsE3oKI"
   },
   "outputs": [],
   "source": [
    "# We can view an individual token by indexing into the Doc object.\n",
    "print(doc[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGapNHYQFYVa"
   },
   "outputs": [],
   "source": [
    "# A Doc object is a container of other objects, namely Token and Span objects.\n",
    "print(type(doc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtL2IgIAGOd9"
   },
   "outputs": [],
   "source": [
    "# Slicing a Doc object returns a Span object.\n",
    "print(doc[0:3])\n",
    "print(type(doc[0:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xybH4jjYGo73"
   },
   "outputs": [],
   "source": [
    "# Access a token's index in a sentence.\n",
    "print([(t.text, t.i) for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TqE980F4Vrt"
   },
   "source": [
    "Spacy's tokenization is _non-destructive_, which means the original input can be reconstructed from the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OjXb8mR_DK-1"
   },
   "outputs": [],
   "source": [
    "# You can view the original input like so:\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokens have many useful properties\n",
    "token = doc[10]\n",
    "print(token)\n",
    "token.is_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentence is automatically parsed into a syntactic dependecy tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc[15])\n",
    "list(doc[15].subtree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73vuSX7MDK79"
   },
   "source": [
    "You can learn more about the **Token** and **Span** objects here:<br>\n",
    "https://spacy.io/api/token<br>\n",
    "https://spacy.io/api/span\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lume_1UP6ySQ"
   },
   "source": [
    "We can also tokenize multiple sentences and access each sentence individually using the **Doc** object's _sents_ property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mPZ86x0hDK4m"
   },
   "outputs": [],
   "source": [
    "s = \"\"\"JUST PUT IN ANOTHER 30K IN NOK CALLS LET'S GO! $GME $NOK BUY AND HOLD üöÄüöÄ üöÄüöÄ. We need to stick together and üíéüñê the ever lovin shit out of this opportunity. We will leave no man or woman behind! Forward! Together!\"\"\"\n",
    "\n",
    "doc = nlp(s)\n",
    "\n",
    "# Look at individual sentences (there should be multiple 'Span' objects).\n",
    "for sent in doc.sents:\n",
    "    print(f\"##### {sent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvSfDUyK06Qg"
   },
   "source": [
    "### Tokenization Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fyywcBrCHzSk"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE:\n",
    "# 1) Tokenize the following text\n",
    "# 2) Iterate through the tokens to check whether there's a currency symbol.\n",
    "# 3) If there is, and the currency label is followed by a number, print\n",
    "#    both the symbol and the number.\n",
    "# \n",
    "# Look through https://spacy.io/api/token#attributes on how to check whether\n",
    "# a token is a currency symbol or a number.\n",
    "#\n",
    "# Expected output: \"$20\".\n",
    "s = \"It's not about the money (only $20.15), it's about sending a message. üöÄüíéüôå\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: find the longest token in the WallStreetBets dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "skajI-OZDK0t"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Learn how the spaCy tokenizer works and how to customize it:\n",
    "# https://spacy.io/usage/linguistic-features#tokenization\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ikbnyb8rDKv9"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Read through spaCy-101 and if you're interested, check out their course\n",
    "# on spaCy itself (link on the page).\n",
    "# https://spacy.io/usage/spacy-101\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMArLP91DKUW"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# OPTIONAL EXERCISE: Look up how to tokenize the sentence below using NLTK. The imports \n",
    "# are done for you. Does the NLTK tokenizer handle \"N.Y.C.\" correctly?\n",
    "#\n",
    "import nltk\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "s = \"Let's go to N.Y.C. for the weekend.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMbm9tTakDdy"
   },
   "source": [
    "**NOTE**: Different tokenizers will give subtly different results based on the rules they use. Experiment with different tokenizers and use the one best suited for your project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUsfYCpVT4nI"
   },
   "source": [
    "# Basic Preprocessing\n",
    "## Case-Folding, Stop Word Removal, Stemming, and Lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgDDrCeI8f-4"
   },
   "source": [
    "spaCy performs all these preprocessing steps (except stemming) behind the scenes for you. Inline with its non-destructive policy, the tokens aren't modified directly. Rather, each **Token** object has a number of attributes which can help you get views of your document with these pre-processing steps applied. The attributes a **Token** has can be found here:<br>\n",
    "https://spacy.io/api/token#attributes\n",
    "<br><br>\n",
    "More information about spaCy's processing pipeline:<br>\n",
    "https://spacy.io/usage/processing-pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDEMR6En1j3H"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "s = \"Once you're done with GME - $AG and $SLV, the gentleman's short squeeze, driven by macro fundementals :/\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwA1ct0obYlR"
   },
   "source": [
    "### Case-Folding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "biBPWrVd9BrK"
   },
   "source": [
    "View your document with case-folding using the *lower_* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1nt4RpzdgQQL"
   },
   "outputs": [],
   "source": [
    "print([t.lower_ for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HL46I4sH9OMq"
   },
   "source": [
    "You can also apply conditions when generating these views. For example, we can skip case-folding if a token is the start of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IO0PQ8IFhOlZ"
   },
   "outputs": [],
   "source": [
    "print([t.lower_ if not t.is_sent_start else t for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7pTz8XJbmaT"
   },
   "source": [
    "### Stop Word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZLqqmHa9cRx"
   },
   "source": [
    "spaCy comes with a default stop word list. To view your document with stop words removed, you can use the *is_stop* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kvXbuDEhOxu"
   },
   "outputs": [],
   "source": [
    "# spaCy's default stop word list.\n",
    "print(nlp.Defaults.stop_words)\n",
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAS1xmgOhO5y"
   },
   "outputs": [],
   "source": [
    "print([t for t in doc if not t.is_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPd1aiLrbqcK"
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKidP32Y_qcE"
   },
   "source": [
    "It's similar with lemmatization. You can view your document with lemmatization applied through the *lemma_* attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fhdRleESkzTu"
   },
   "outputs": [],
   "source": [
    "[(t.text, t.lemma_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note `fundementals` which is misspelled. Lemmatisation still works although of course the lemma is wrong too !\n",
    "\n",
    "*Question:* Why would we do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wOXJI061npqN"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Find out how to add and remove your own stop words in spaCy. Add the \n",
    "# word 'told' as a stop word, test that it works, then remove it from \n",
    "# the stop word list.\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLcCYIy-lP1u"
   },
   "outputs": [],
   "source": [
    "#\n",
    "# EXERCISE: Read up on how to add your own custom attributes to Token objects\n",
    "# and try adding one of your own.\n",
    "# https://spacy.io/usage/processing-pipelines#custom-components-attributes\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9HLYYUt1kOP"
   },
   "source": [
    "## Named Entity Recognition, and Parsing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tr5SqjHwSWpI"
   },
   "source": [
    "spaCy performs Part-of-Speech (POS) tagging, Named Entity Recognition (NER), and parsing as part of its default pipeline in the *nlp* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "shgWRMCq1kmy"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "s = \"Once you're done with GME, $AG and $SLV, the gentleman's short squeeze, driven by macro fundamentals.\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jte6K6HJb750"
   },
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2J2BjPyqWFEf"
   },
   "source": [
    "There are multiple ways to access named entities. One way is through the *ent_type_* attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dWjNrX6koNVj"
   },
   "outputs": [],
   "source": [
    "\n",
    "[(t.text, t.ent_type_) for t in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7p8IcNGpBTP"
   },
   "source": [
    "So let's find all the ticker symbols in this sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aBng8zdvjly"
   },
   "outputs": [],
   "source": [
    "print([(t.text, t.ent_type_) for t in doc if t.ent_type_ != \"\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lNS65_2XJIY"
   },
   "source": [
    "Another way is through the _ents_ property of the **Doc** object. Here, we iterate through _ents_ and print the entity itself and its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kSCzxs02vjdL"
   },
   "outputs": [],
   "source": [
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHfZmta8XX9Y"
   },
   "source": [
    "Note how `GME - $AG` is outputted above as a single span when you use _ents_.\n",
    "<br><br>\n",
    "You can also access the positions of entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mSzwRD0MvjTN"
   },
   "outputs": [],
   "source": [
    "print([(ent.text, ent.label_, ent.start_char, ent.end_char) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvvQ9_7FdEHT"
   },
   "source": [
    "spaCy is bundled with visualizers for both parsing and named entities.<br>\n",
    "https://spacy.io/usage/visualizers\n",
    "<br><br>\n",
    "Here, we visualize the entities in our sample sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87eLywmVZCdw"
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# We need to set the 'jupyter' variable to True in order to output\n",
    "# the visualization directly. Otherwise, you'll get raw HTML.\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFXPL37Rg2xm"
   },
   "source": [
    "### Fine-tuning models\n",
    "You can fine-tune any NER model too in spaCy. Read the docs if you need that!\n",
    "### Using spaCy's Matcher to find patterns\n",
    "spaCy comes with a host of pattern-matching functionality. Beyond regex, spaCy can match on a variety of attributes such as POS tags, entity labels, lemmas, dependencies, entire phrases, and a lot more. You can learn more here:<br>\n",
    "https://spacy.io/usage/rule-based-matching<br>\n",
    "https://explosion.ai/demos/matcher\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nhN7p9G8taJ"
   },
   "source": [
    "# Additional Reading and Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bM4G2KWa8wXO"
   },
   "source": [
    "\n",
    "- https://www.nlpdemystified.org/course/tokenization\n",
    "- https://spacy.io/usage/processing-pipelines\n",
    "- Take the free and succinct spaCy course (available in multiple languages):<br>\n",
    "https://course.spacy.io/\n",
    "- https://spacy.io/usage/spacy-101\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "nlpdemystified-preprocessing.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
